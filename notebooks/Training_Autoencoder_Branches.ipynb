{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:823: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Mapping\", _collections.Mapping)\n",
      "C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\data_structures.py:312: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class _ListWrapper(List, collections.MutableSequence,\n",
      "C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\util.py:448: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class _ObjectIdentitySet(collections.MutableSet):\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\keras\\callbacks\\callbacks.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "path.append('../src/')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import pickle as pkl\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, GlobalAvgPool2D\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from global_config import FILE_DUMP_IMAGES, FILE_DUMP_MRKS, FOLDER_MODELS \n",
    "from global_config import RANDOM_SEED, IMAGE_SIZE, USER_IDS\n",
    "from utils import load, plot\n",
    "from custom_metrics import precision, recall, f1, specificity \n",
    "from custom_metrics import negative_predictive_value as npv \n",
    "from custom_metrics import matthews_correlation_coefficient as mcc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pybossa/pybossa_images.pkl\n",
      "../data/pybossa/pybossa_mrks.pkl\n",
      "../models/autoencoders_requirements/\n",
      "(224, 224)\n",
      "42\n",
      "best_branches\n"
     ]
    }
   ],
   "source": [
    "print(FILE_DUMP_IMAGES)\n",
    "print(FILE_DUMP_MRKS)\n",
    "print(FOLDER_MODELS)\n",
    "print(IMAGE_SIZE)\n",
    "print(RANDOM_SEED)\n",
    "print(USER_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = 'Autoencoder Requirements'\n",
    "\n",
    "FILE_BASE_MODEL = '../models/autoencoders_unsupervised/model_2020_03_15-10_43_27.h5'\n",
    "FILE_MERGED_MODEL = f'{FOLDER_MODELS}merged_model_{USER_IDS}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_model_name(prefix='model_', suffix='', format='%Y_%m_%d-%H_%M_%S', ext='.h5'):\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(format)\n",
    "    return f'{prefix}{timestamp}{suffix}{ext}'\n",
    "\n",
    "def set_random_seeds():\n",
    "    os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "    rn.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "def plot_metrics(y_true, y_pred, hist):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred) \n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    accuracy_sk = accuracy_score(y_true, y_pred)\n",
    "    precision_sk = precision_score(y_true, y_pred)\n",
    "    recall_sk = recall_score(y_true, y_pred)\n",
    "    f_measure = f1_score(y_true, y_pred)\n",
    "    specificity_sk = tn / (tn + fp + 1e-7)\n",
    "    npv_sk = tn / (tn + fn + 1e-7)\n",
    "    \n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    mcc_sk = num / np.sqrt(den + 1e-7)\n",
    "    \n",
    "    print()\n",
    "    print('   Final Accuracy: {:=6.2f}%'.format(accuracy_sk * 100))\n",
    "    print('  Final Precision: {:=6.2f}%'.format(precision_sk * 100))\n",
    "    print('     Final Recall: {:=6.2f}%'.format(recall_sk * 100))\n",
    "    print('  Final F-measure: {:=6.2f}%'.format(f_measure * 100))\n",
    "    print('Final Specificity: {:=6.2f}%'.format(specificity_sk * 100))\n",
    "    print('        Final NPV: {:=6.2f}%'.format(npv_sk * 100))\n",
    "    print('        Final MCC: {:=6.2f}%'.format(mcc_sk * 100))    \n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    plot.keras_hist(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, name):\n",
    "    set_random_seeds()\n",
    "\n",
    "    outputs = Dense(units=64, activation='relu', name=f'{name}_1')(base_model.output)\n",
    "    outputs = Dense(units=32, activation='relu', name=f'{name}_2')(outputs)\n",
    "    outputs = Dense(units=1, activation='sigmoid', name=f'{name}')(outputs)\n",
    "    model = Model(inputs=base_model.inputs, outputs=outputs)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = (name in layer.name)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy', precision, recall, f1, specificity, npv, mcc]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, name):\n",
    "    FILE_MODEL = FOLDER_MODELS + timestamp_model_name(prefix=f'{name}_')\n",
    "    metric_to_monitor = 'val_matthews_correlation_coefficient'\n",
    "    \n",
    "    list_callbacks = [\n",
    "        EarlyStopping(monitor=metric_to_monitor, mode='max', patience=30, verbose=1, restore_best_weights=True),\n",
    "        ModelCheckpoint(FILE_MODEL, monitor=metric_to_monitor, mode='max', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight('balanced', classes, y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "    u_train, c_train = np.unique(y_train, return_counts=True)\n",
    "    u_val, c_val = np.unique(y_val, return_counts=True)\n",
    "    \n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "    mlflow.log_param(\"test_size\", TEST_SIZE)\n",
    "    mlflow.log_param(\"seed\", RANDOM_SEED)\n",
    "    mlflow.log_param(\"class_train\", dict(zip(u_train, c_train)))\n",
    "    mlflow.log_param(\"class_val\", dict(zip(u_val, c_val)))\n",
    "    mlflow.log_param(\"class_train_prop\", dict(zip(u_train, c_train / c_train.sum())))\n",
    "    mlflow.log_param(\"class_val_prop\", dict(zip(u_val, c_val / c_val.sum())))\n",
    "    mlflow.set_tag(\"requirement\", name)\n",
    "    mlflow.set_tag(\"user_ids\", USER_IDS)\n",
    "    mlflow.keras.autolog()\n",
    "\n",
    "    hist = model.fit(\n",
    "        x_train / 255, \n",
    "        y_train, \n",
    "        batch_size=32, \n",
    "        epochs=100, \n",
    "        validation_data=(x_val / 255, y_val),\n",
    "        class_weight=class_weights,\n",
    "        callbacks=list_callbacks\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    return hist, FILE_MODEL\n",
    "\n",
    "\n",
    "def train_requirement_branch(base_model, x, y, name):\n",
    "    classes, counts = np.unique(y, return_counts=True) \n",
    "    if len(classes) == 1:\n",
    "        print(f\"the requirement {name} has only one label.\")\n",
    "        return None\n",
    "    \n",
    "    if min(counts) == 1:\n",
    "        print(f\"the requirement {name} has only one sample for one of the classes.\")\n",
    "        return None\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x, y, \n",
    "        test_size=TEST_SIZE, \n",
    "        stratify=y, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    model = build_model(base_model, name)\n",
    "    hist, file_model = train_model(model, x_train, y_train, x_val, y_val, name)\n",
    "    \n",
    "    y_pred = model.predict(x_val / 255)\n",
    "    y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "    plot_metrics(y_val, y_pred, hist)\n",
    "\n",
    "    return file_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(FILE_BASE_MODEL)\n",
    "\n",
    "embeddings = GlobalAvgPool2D(name='embeddings')(model.get_layer(name='encoded').output)\n",
    "\n",
    "encoder = Model(inputs=model.inputs, outputs=embeddings)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = pkl.load(open(FILE_DUMP_IMAGES, 'rb'))\n",
    "mrks, _ = pkl.load(open(FILE_DUMP_MRKS, 'rb'))\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(mrks.shape, mrks.dtype)\n",
    "\n",
    "list_model_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.blurred.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'blurred')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Looking Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.looking_away.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'looking_away')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ink Marked/Creased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.ink_marked_creased.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'ink_marked_creased')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Unnatural Skin Tone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.unnatural_skin_tone.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'unnatural_skin_tone')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Too Dark/Light "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.too_dark_light.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'too_dark_light')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Washed Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.washed_out.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'washed_out')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Pixelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.pixelation.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'pixelation')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Hair Across Eyes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.hair_across_eyes.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'hair_across_eyes')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Eyes Closed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.eyes_closed.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'eyes_closed')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Varied Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.varied_background.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'varied_background')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Roll/pitch/yaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.roll_pitch_yaw.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'roll_pitch_yaw')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Flash Reflection on Skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.flash_reflection_on_skin.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'flash_reflection_on_skin')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Red Eyes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.red_eyes.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'red_eyes')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Shadows Behind Head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.shadows_behind_head.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'shadows_behind_head')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Shadows Across Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.shadows_across_face.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'shadows_across_face')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. Dark Tinted Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.dark_tinted_lenses.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'dark_tinted_lenses')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. Flash Reflection on Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.flash_reflection_on_lenses.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'flash_reflection_on_lenses')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19. Frames Too Heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.frames_too_heavy.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'frames_too_heavy')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Frame Covering Eyes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.frame_covering_eyes.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'frame_covering_eyes')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Hat/cap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.hat_cap.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'hat_cap')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *22. Veil Over Face*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.veil_over_face.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'veil_over_face')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23. Mouth Open "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.mouth_open.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'mouth_open')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *24. Presence of Other Faces or Toys too Close to Face*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array([mrk.photo_reqs.presence_of_other_faces_or_toys.value for mrk in mrks])\n",
    "\n",
    "model_file = train_requirement_branch(encoder, x, y, 'presence_of_other_faces_or_toys')\n",
    "list_model_files.append(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_requirement_branch_to_base_model(base_model_last_layer, req_model):\n",
    "    req_layers = [layer for layer in req_model.layers if isinstance(layer, Dense)]\n",
    "\n",
    "    outputs = base_model_last_layer\n",
    "    for layer in req_layers:\n",
    "        outputs = req_model.get_layer(layer.name)(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_models = np.array([file for file in list_model_files if file is not None])\n",
    "\n",
    "print(file_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../models/autoencoders_requirements/blurred_2020_03_19-22_35_06.h5'\n",
      " '../models/autoencoders_requirements/looking_away_2020_03_29-00_50_08.h5'\n",
      " '../models/autoencoders_requirements/ink_marked_creased_2020_03_19-22_57_13.h5'\n",
      " '../models/autoencoders_requirements/unnatural_skin_tone_2020_03_15-16_34_22.h5'\n",
      " '../models/autoencoders_requirements/too_dark_light_2020_03_18-20_38_12.h5'\n",
      " '../models/autoencoders_requirements/washed_out_2020_03_15-16_58_44.h5'\n",
      " '../models/autoencoders_requirements/pixelation_2020_03_19-23_31_40.h5'\n",
      " '../models/autoencoders_requirements/hair_across_eyes_2020_03_15-17_17_55.h5'\n",
      " '../models/autoencoders_requirements/eyes_closed_2020_03_15-17_33_05.h5'\n",
      " '../models/autoencoders_requirements/varied_background_2020_03_20-00_03_35.h5'\n",
      " '../models/autoencoders_requirements/roll_pitch_yaw_2020_03_29-23_59_19.h5'\n",
      " '../models/autoencoders_requirements/flash_reflection_on_skin_2020_03_27-20_39_35.h5'\n",
      " '../models/autoencoders_requirements/red_eyes_2020_03_27-20_54_25.h5'\n",
      " '../models/autoencoders_requirements/shadows_behind_head_2020_03_21-14_27_57.h5'\n",
      " '../models/autoencoders_requirements/shadows_across_face_2020_03_20-01_09_25.h5'\n",
      " '../models/autoencoders_requirements/dark_tinted_lenses_2020_03_20-01_17_59.h5'\n",
      " '../models/autoencoders_requirements/flash_reflection_on_lenses_2020_03_22-02_34_15.h5'\n",
      " '../models/autoencoders_requirements/frames_too_heavy_2020_03_22-02_49_07.h5'\n",
      " '../models/autoencoders_requirements/frame_covering_eyes_2020_03_30-22_19_54.h5'\n",
      " '../models/autoencoders_requirements/hat_cap_2020_03_18-23_56_17.h5'\n",
      " '../models/autoencoders_requirements/veil_over_face_2020_03_20-02_16_35.h5'\n",
      " '../models/autoencoders_requirements/mouth_open_2020_03_25-23_23_28.h5'\n",
      " '../models/autoencoders_requirements/presence_of_other_faces_or_toys_2020_03_27-23_02_46.h5']\n"
     ]
    }
   ],
   "source": [
    "file_models = np.array([\n",
    "    FOLDER_MODELS + 'blurred_2020_03_19-22_35_06.h5',\n",
    "    FOLDER_MODELS + 'looking_away_2020_03_29-00_50_08.h5',\n",
    "    FOLDER_MODELS + 'ink_marked_creased_2020_03_19-22_57_13.h5',\n",
    "    FOLDER_MODELS + 'unnatural_skin_tone_2020_03_15-16_34_22.h5',\n",
    "    FOLDER_MODELS + 'too_dark_light_2020_03_18-20_38_12.h5',\n",
    "    FOLDER_MODELS + 'washed_out_2020_03_15-16_58_44.h5',\n",
    "    FOLDER_MODELS + 'pixelation_2020_03_19-23_31_40.h5',\n",
    "    FOLDER_MODELS + 'hair_across_eyes_2020_03_15-17_17_55.h5',\n",
    "    FOLDER_MODELS + 'eyes_closed_2020_03_15-17_33_05.h5',\n",
    "    FOLDER_MODELS + 'varied_background_2020_03_20-00_03_35.h5',\n",
    "    FOLDER_MODELS + 'roll_pitch_yaw_2020_03_29-23_59_19.h5',\n",
    "    FOLDER_MODELS + 'flash_reflection_on_skin_2020_03_27-20_39_35.h5',\n",
    "    FOLDER_MODELS + 'red_eyes_2020_03_27-20_54_25.h5',\n",
    "    FOLDER_MODELS + 'shadows_behind_head_2020_03_21-14_27_57.h5',\n",
    "    FOLDER_MODELS + 'shadows_across_face_2020_03_20-01_09_25.h5',\n",
    "    FOLDER_MODELS + 'dark_tinted_lenses_2020_03_20-01_17_59.h5',\n",
    "    FOLDER_MODELS + 'flash_reflection_on_lenses_2020_03_22-02_34_15.h5',\n",
    "    FOLDER_MODELS + 'frames_too_heavy_2020_03_22-02_49_07.h5',\n",
    "    FOLDER_MODELS + 'frame_covering_eyes_2020_03_30-22_19_54.h5',\n",
    "    FOLDER_MODELS + 'hat_cap_2020_03_18-23_56_17.h5',\n",
    "    FOLDER_MODELS + 'veil_over_face_2020_03_20-02_16_35.h5',\n",
    "    FOLDER_MODELS + 'mouth_open_2020_03_25-23_23_28.h5',\n",
    "    FOLDER_MODELS + 'presence_of_other_faces_or_toys_2020_03_27-23_02_46.h5',\n",
    "])\n",
    "\n",
    "print(file_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\numpy\\lib\\type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\arnal\\Miniconda3\\envs\\dl-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_model(FILE_BASE_MODEL)\n",
    "\n",
    "embeddings = GlobalAvgPool2D(name='embeddings')(autoencoder.get_layer(name='encoded').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\n",
    "    'recall': recall,\n",
    "    'precision': precision,\n",
    "    'f1': f1,\n",
    "    'specificity': specificity,\n",
    "    'negative_predictive_value': npv,\n",
    "    'matthews_correlation_coefficient': mcc\n",
    "}\n",
    "\n",
    "list_models = [load_model(file, custom_objects=custom_objects) for file in file_models]\n",
    "list_outputs = [add_requirement_branch_to_base_model(embeddings, model) for model in list_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 224, 224, 16) 448         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn_1 (BatchNormalization)       (None, 224, 224, 16) 64          conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_1 (Activation)             (None, 224, 224, 16) 0           bn_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "pool_1 (MaxPooling2D)           (None, 112, 112, 16) 0           relu_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 112, 112, 32) 4640        pool_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_2 (BatchNormalization)       (None, 112, 112, 32) 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_2 (Activation)             (None, 112, 112, 32) 0           bn_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "pool_2 (MaxPooling2D)           (None, 56, 56, 32)   0           relu_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 56, 56, 64)   18496       pool_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_3 (BatchNormalization)       (None, 56, 56, 64)   256         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_3 (Activation)             (None, 56, 56, 64)   0           bn_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "pool_3 (MaxPooling2D)           (None, 28, 28, 64)   0           relu_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 28, 28, 128)  73856       pool_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_4 (BatchNormalization)       (None, 28, 28, 128)  512         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_4 (Activation)             (None, 28, 28, 128)  0           bn_4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "pool_4 (MaxPooling2D)           (None, 14, 14, 128)  0           relu_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 14, 14, 256)  295168      pool_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_5 (BatchNormalization)       (None, 14, 14, 256)  1024        conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoded (Activation)            (None, 14, 14, 256)  0           bn_5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embeddings (GlobalAveragePoolin (None, 256)          0           encoded[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "blurred_1 (Dense)               (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "looking_away_1 (Dense)          (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ink_marked_creased_1 (Dense)    (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "unnatural_skin_tone_1 (Dense)   (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "too_dark_light_1 (Dense)        (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "washed_out_1 (Dense)            (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pixelation_1 (Dense)            (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "hair_across_eyes_1 (Dense)      (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "eyes_closed_1 (Dense)           (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "varied_background_1 (Dense)     (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "roll_pitch_yaw_1 (Dense)        (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flash_reflection_on_skin_1 (Den (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "red_eyes_1 (Dense)              (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "shadows_behind_head_1 (Dense)   (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "shadows_across_face_1 (Dense)   (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dark_tinted_lenses_1 (Dense)    (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flash_reflection_on_lenses_1 (D (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "frames_too_heavy_1 (Dense)      (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "frame_covering_eyes_1 (Dense)   (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "hat_cap_1 (Dense)               (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "veil_over_face_1 (Dense)        (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mouth_open_1 (Dense)            (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "presence_of_other_faces_or_toys (None, 64)           16448       embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "blurred_2 (Dense)               (None, 32)           2080        blurred_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "looking_away_2 (Dense)          (None, 32)           2080        looking_away_1[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ink_marked_creased_2 (Dense)    (None, 32)           2080        ink_marked_creased_1[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "unnatural_skin_tone_2 (Dense)   (None, 32)           2080        unnatural_skin_tone_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "too_dark_light_2 (Dense)        (None, 32)           2080        too_dark_light_1[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "washed_out_2 (Dense)            (None, 32)           2080        washed_out_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pixelation_2 (Dense)            (None, 32)           2080        pixelation_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hair_across_eyes_2 (Dense)      (None, 32)           2080        hair_across_eyes_1[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "eyes_closed_2 (Dense)           (None, 32)           2080        eyes_closed_1[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "varied_background_2 (Dense)     (None, 32)           2080        varied_background_1[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "roll_pitch_yaw_2 (Dense)        (None, 32)           2080        roll_pitch_yaw_1[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flash_reflection_on_skin_2 (Den (None, 32)           2080        flash_reflection_on_skin_1[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "red_eyes_2 (Dense)              (None, 32)           2080        red_eyes_1[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "shadows_behind_head_2 (Dense)   (None, 32)           2080        shadows_behind_head_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "shadows_across_face_2 (Dense)   (None, 32)           2080        shadows_across_face_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dark_tinted_lenses_2 (Dense)    (None, 32)           2080        dark_tinted_lenses_1[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flash_reflection_on_lenses_2 (D (None, 32)           2080        flash_reflection_on_lenses_1[1][0\n",
      "__________________________________________________________________________________________________\n",
      "frames_too_heavy_2 (Dense)      (None, 32)           2080        frames_too_heavy_1[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "frame_covering_eyes_2 (Dense)   (None, 32)           2080        frame_covering_eyes_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hat_cap_2 (Dense)               (None, 32)           2080        hat_cap_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "veil_over_face_2 (Dense)        (None, 32)           2080        veil_over_face_1[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mouth_open_2 (Dense)            (None, 32)           2080        mouth_open_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "presence_of_other_faces_or_toys (None, 32)           2080        presence_of_other_faces_or_toys_1\n",
      "__________________________________________________________________________________________________\n",
      "blurred (Dense)                 (None, 1)            33          blurred_2[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "looking_away (Dense)            (None, 1)            33          looking_away_2[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ink_marked_creased (Dense)      (None, 1)            33          ink_marked_creased_2[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "unnatural_skin_tone (Dense)     (None, 1)            33          unnatural_skin_tone_2[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "too_dark_light (Dense)          (None, 1)            33          too_dark_light_2[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "washed_out (Dense)              (None, 1)            33          washed_out_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pixelation (Dense)              (None, 1)            33          pixelation_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hair_across_eyes (Dense)        (None, 1)            33          hair_across_eyes_2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "eyes_closed (Dense)             (None, 1)            33          eyes_closed_2[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "varied_background (Dense)       (None, 1)            33          varied_background_2[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "roll_pitch_yaw (Dense)          (None, 1)            33          roll_pitch_yaw_2[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flash_reflection_on_skin (Dense (None, 1)            33          flash_reflection_on_skin_2[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "red_eyes (Dense)                (None, 1)            33          red_eyes_2[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "shadows_behind_head (Dense)     (None, 1)            33          shadows_behind_head_2[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "shadows_across_face (Dense)     (None, 1)            33          shadows_across_face_2[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dark_tinted_lenses (Dense)      (None, 1)            33          dark_tinted_lenses_2[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flash_reflection_on_lenses (Den (None, 1)            33          flash_reflection_on_lenses_2[1][0\n",
      "__________________________________________________________________________________________________\n",
      "frames_too_heavy (Dense)        (None, 1)            33          frames_too_heavy_2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "frame_covering_eyes (Dense)     (None, 1)            33          frame_covering_eyes_2[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hat_cap (Dense)                 (None, 1)            33          hat_cap_2[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "veil_over_face (Dense)          (None, 1)            33          veil_over_face_2[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mouth_open (Dense)              (None, 1)            33          mouth_open_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "presence_of_other_faces_or_toys (None, 1)            33          presence_of_other_faces_or_toys_2\n",
      "==================================================================================================\n",
      "Total params: 821,495\n",
      "Trainable params: 820,503\n",
      "Non-trainable params: 992\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merged_model = Model(inputs=autoencoder.inputs, outputs=list_outputs)\n",
    "merged_model.summary()\n",
    "\n",
    "merged_model.save(FILE_MERGED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pybossa/images/AR_FDB_m-013-17.png\n",
      "1 of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.99974114]], dtype=float32),\n",
       " array([[0.87507546]], dtype=float32),\n",
       " array([[0.4049703]], dtype=float32),\n",
       " array([[0.9546674]], dtype=float32),\n",
       " array([[0.98410183]], dtype=float32),\n",
       " array([[1.]], dtype=float32),\n",
       " array([[0.87229383]], dtype=float32),\n",
       " array([[0.82480884]], dtype=float32),\n",
       " array([[0.7220866]], dtype=float32),\n",
       " array([[0.47669944]], dtype=float32),\n",
       " array([[0.8694878]], dtype=float32),\n",
       " array([[0.8720084]], dtype=float32),\n",
       " array([[0.5726314]], dtype=float32),\n",
       " array([[0.9984843]], dtype=float32),\n",
       " array([[0.971915]], dtype=float32),\n",
       " array([[0.97836375]], dtype=float32),\n",
       " array([[0.2243136]], dtype=float32),\n",
       " array([[1.]], dtype=float32),\n",
       " array([[0.9240353]], dtype=float32),\n",
       " array([[0.9999976]], dtype=float32),\n",
       " array([[0.9992687]], dtype=float32),\n",
       " array([[0.49474713]], dtype=float32),\n",
       " array([[1.]], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "# list_files = glob('../data/pybossa/images/*')\n",
    "list_files = ['../data/pybossa/images/AR_FDB_m-013-17.png']\n",
    "random_file = np.random.choice(list_files)\n",
    "print(random_file)\n",
    "\n",
    "im = load.images_from_list_files([random_file], output_size=IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
    "im /= 255\n",
    "\n",
    "y_pred = merged_model.predict(im)\n",
    "list_predictions = [model.predict(im) for model in list_models]\n",
    "assert(np.all([np.allclose(pred1, pred2) for pred1, pred2 in zip(y_pred, list_predictions)]))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
