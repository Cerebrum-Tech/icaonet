{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636f9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "import os\n",
    "path.append(os.path.abspath('../src'))\n",
    "\n",
    "# mandatory imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from data_structures import Point, Rect\n",
    "\n",
    "# optional imports\n",
    "import matplotlib.pyplot as plt\n",
    "from iso_standard import PhotographicRequirements\n",
    "\n",
    "# Disable eager execution (optional but recommended for using the Session API)\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd5133",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ececb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_pb(pb_file):\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(open(pb_file, 'rb').read())\n",
    "\n",
    "    with graph.as_default():\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")\n",
    "    return graph\n",
    "\n",
    "def run_graph(graph, inputs_list, input_tensor_names, output_tensor_names):\n",
    "    assert len(inputs_list) == len(input_tensor_names)\n",
    "\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:\n",
    "        input_tensors = [graph.get_tensor_by_name(name) for name in input_tensor_names]\n",
    "        output_tensors = [graph.get_tensor_by_name(name) for name in output_tensor_names]\n",
    "        feed_dict = {tensor: value for tensor, value in zip(input_tensors, inputs_list)}\n",
    "        return sess.run(output_tensors, feed_dict=feed_dict)\n",
    "\n",
    "def run_face_detector(bgr_image):\n",
    "    img = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.uint8)\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    inputs_list = [np.expand_dims(img, axis=0)]\n",
    "    input_tensor_names = [\"image_tensor:0\"]\n",
    "    output_tensor_names = [\"detection_boxes:0\", \"detection_scores:0\"]\n",
    "    boxes, scores = run_graph(\n",
    "        face_detector, inputs_list, input_tensor_names, output_tensor_names\n",
    "    )\n",
    "\n",
    "    best_rect = boxes.squeeze()[scores.squeeze().argmax()]\n",
    "    best_rect = best_rect * [height, width, height, width]\n",
    "    best_rect = best_rect.astype(np.int32)  # Use np.int32 to avoid warnings\n",
    "    y1, x1, y2, x2 = best_rect\n",
    "    return Rect.from_tl_br_points((x1, y1), (x2, y2))\n",
    "\n",
    "def preprocessing(bgr_image):\n",
    "    face_rect = run_face_detector(bgr_image)\n",
    "    w, h = face_rect.width, face_rect.height\n",
    "\n",
    "    pad_rect = Rect(face_rect.x, face_rect.y, w + int(w * 1.5), h + int(h * 1.5))\n",
    "    pad_rect.height = pad_rect.width = max(pad_rect.height, pad_rect.width)\n",
    "\n",
    "    center_pad = Point(\n",
    "        (pad_rect.x + pad_rect.width) // 2, (pad_rect.y + pad_rect.height) // 2\n",
    "    )\n",
    "    center_face = Point(\n",
    "        (face_rect.x + face_rect.width) // 2, (face_rect.y + face_rect.height) // 2\n",
    "    )\n",
    "    offset_center = Point(center_face.x - center_pad.x, center_face.y - center_pad.y)\n",
    "    pad_rect += offset_center\n",
    "\n",
    "    im_h, im_w = bgr_image.shape[:2]\n",
    "    br_x, br_y = pad_rect.br()\n",
    "    left = abs(min(0, pad_rect.x))\n",
    "    top = abs(min(0, pad_rect.y))\n",
    "    right = 0 if br_x < im_w else br_x - im_w\n",
    "    bottom = 0 if br_y < im_h else br_y - im_h\n",
    "\n",
    "    im_res = cv2.copyMakeBorder(\n",
    "        bgr_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0\n",
    "    )\n",
    "    pad_rect.x += left\n",
    "    pad_rect.y += top\n",
    "\n",
    "    l, t = pad_rect.tl()\n",
    "    r, b = pad_rect.br()\n",
    "    return im_res[t:b, l:r]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73bafe",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b1e8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = load_graph_from_pb(\"../resources/models/face_detector.pb\")\n",
    "icaonet = load_model(\"../resources/models/icaonet.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c793dc8",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a21f75da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@243.770] global loadsave.cpp:241 findDecoder imread_('C://Users//arnal//Desktop//doutorado//FVC/afwDB_0.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC://Users//arnal//Desktop//doutorado//FVC/afwDB_0.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m im \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(filepath, cv2\u001b[38;5;241m.\u001b[39mIMREAD_ANYCOLOR)\n\u001b[0;32m----> 4\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m im \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(im, (\u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(im, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mpreprocessing\u001b[0;34m(bgr_image)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessing\u001b[39m(bgr_image):\n\u001b[0;32m---> 39\u001b[0m     face_rect \u001b[38;5;241m=\u001b[39m \u001b[43mrun_face_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbgr_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m face_rect\u001b[38;5;241m.\u001b[39mwidth, face_rect\u001b[38;5;241m.\u001b[39mheight\n\u001b[1;32m     42\u001b[0m     pad_rect \u001b[38;5;241m=\u001b[39m Rect(face_rect\u001b[38;5;241m.\u001b[39mx, face_rect\u001b[38;5;241m.\u001b[39my, w \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(w \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m), h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(h \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m, in \u001b[0;36mrun_face_detector\u001b[0;34m(bgr_image)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_face_detector\u001b[39m(bgr_image):\n\u001b[0;32m---> 21\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbgr_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     23\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/Users/alperenozalp/git/icaonet/images (2).jpeg\"\n",
    "\n",
    "im = cv2.imread(filepath, cv2.IMREAD_ANYCOLOR)\n",
    "im = preprocessing(im)\n",
    "im = cv2.resize(im, (160, 160), interpolation=cv2.INTER_AREA).astype(np.float32)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = im / 255\n",
    "\n",
    "plt.imshow(im[0, :, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64809203, 0.9347415 , 0.99573874, 0.08281666, 0.991169  ,\n",
       "        0.9998725 , 0.9772672 , 0.27883738, 0.96358013, 0.05340803,\n",
       "        0.2058002 , 0.14253008, 0.9828384 , 0.47127613, 0.82925045,\n",
       "        0.9903072 , 0.75461185, 0.97962976, 0.8930521 , 0.6584907 ,\n",
       "        0.9751569 , 0.827954  , 0.9905076 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = icaonet.predict([im])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a2082",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efd0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02] Blurred: 0.6480920314788818\n",
      "[03] Looking away: 0.9347414970397949\n",
      "[04] Ink marked/creased: 0.9957387447357178\n",
      "[05] Unnatural skin tone: 0.08281666040420532\n",
      "[06] Too dark/light: 0.9911689758300781\n",
      "[07] Washed out: 0.9998725056648254\n",
      "[08] Pixelation: 0.9772672057151794\n",
      "[09] Hair across eyes: 0.2788373827934265\n",
      "[10] Eyes closed: 0.9635801315307617\n",
      "[11] Varied background: 0.053408026695251465\n",
      "[12] Roll/pitch/yaw rotations greater than a predefined thresholds: 0.20580020546913147\n",
      "[13] Flash reflection on skin: 0.14253008365631104\n",
      "[14] Red eyes: 0.9828383922576904\n",
      "[15] Shadows behind head: 0.4712761342525482\n",
      "[16] Shadows across face: 0.8292504549026489\n",
      "[17] Dark tinted lenses: 0.9903072118759155\n",
      "[18] Flash reflection on lenses: 0.7546118497848511\n",
      "[19] Frames too heavy: 0.9796297550201416\n",
      "[20] Frame covering eyes: 0.8930521011352539\n",
      "[21] Hat/cap: 0.6584907174110413\n",
      "[22] Veil over face: 0.9751569032669067\n",
      "[23] Mouth open: 0.8279539942741394\n",
      "[24] Presence of other faces or toys too close to face: 0.9905076026916504\n"
     ]
    }
   ],
   "source": [
    "# optional\n",
    "reqs = PhotographicRequirements(*y_pred.squeeze())\n",
    "print(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64809203\n"
     ]
    }
   ],
   "source": [
    "# you can get the score for each individual requirement\n",
    "print(reqs.blurred.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32013146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64809203\n",
      "0.9347415\n",
      "0.99573874\n",
      "0.08281666\n",
      "0.991169\n",
      "0.9998725\n",
      "0.9772672\n",
      "0.27883738\n",
      "0.96358013\n",
      "0.053408027\n",
      "0.2058002\n",
      "0.14253008\n",
      "0.9828384\n",
      "0.47127613\n",
      "0.82925045\n",
      "0.9903072\n",
      "0.75461185\n",
      "0.97962976\n",
      "0.8930521\n",
      "0.6584907\n",
      "0.9751569\n",
      "0.827954\n",
      "0.9905076\n"
     ]
    }
   ],
   "source": [
    "# you can also iter through requirements\n",
    "for req in reqs:\n",
    "    print(req.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a9baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# useful methods\n",
    "print(reqs.blurred.is_compliant(threshold=0.5)) # default threshold = 0.5\n",
    "print(reqs.varied_background.is_non_compliant(threshold=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86adc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
